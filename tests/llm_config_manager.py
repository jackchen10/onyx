#!/usr/bin/env python3
"""
Onyx LLMé…ç½®ç®¡ç†è„šæœ¬
ç®¡ç†å’Œé…ç½®å¤§è¯­è¨€æ¨¡å‹æä¾›å•†
"""

import os
import json
import requests
import sys
from typing import Dict, List

class LLMConfigManager:
    def __init__(self):
        self.backend_url = "http://localhost:8080"
        self.supported_providers = {
            "openai": {
                "display_name": "OpenAI",
                "api_key_required": True,
                "default_model": "gpt-4o",
                "fast_model": "gpt-4o-mini",
                "models": ["o1", "o3-mini", "gpt-4o", "gpt-4o-mini", "gpt-4", "gpt-3.5-turbo"]
            },
            "anthropic": {
                "display_name": "Anthropic",
                "api_key_required": True,
                "default_model": "claude-3-7-sonnet-20250219",
                "fast_model": "claude-3-5-sonnet-20241022",
                "models": ["claude-3-7-sonnet-20250219", "claude-3-5-sonnet-20241022", "claude-3-opus-20240229"]
            },
            "azure": {
                "display_name": "Azure OpenAI",
                "api_key_required": True,
                "api_base_required": True,
                "api_version_required": True,
                "deployment_name_required": True,
                "models": ["gpt-4o", "gpt-4", "gpt-3.5-turbo"]
            },
            "bedrock": {
                "display_name": "AWS Bedrock",
                "api_key_required": False,
                "default_model": "anthropic.claude-3-5-sonnet-20241022-v2:0",
                "fast_model": "anthropic.claude-3-5-sonnet-20241022-v2:0",
                "custom_config": ["AWS_REGION_NAME", "AWS_ACCESS_KEY_ID", "AWS_SECRET_ACCESS_KEY"],
                "models": ["anthropic.claude-3-5-sonnet-20241022-v2:0", "meta.llama3-1-70b-instruct-v1:0"]
            },
            "vertex_ai": {
                "display_name": "GCP Vertex AI",
                "api_key_required": False,
                "default_model": "gemini-2.0-flash",
                "fast_model": "gemini-2.0-flash-lite",
                "custom_config": ["VERTEX_CREDENTIALS_FILE", "VERTEX_LOCATION"],
                "models": ["gemini-2.0-flash", "gemini-2.0-flash-lite", "gemini-1.5-pro", "claude-sonnet-4"]
            }
        }
    
    def print_header(self, title: str):
        """æ‰“å°æ ‡é¢˜"""
        print(f"\n{'='*60}")
        print(f"ğŸ¤– {title}")
        print('='*60)
    
    def get_current_providers(self) -> List[Dict]:
        """è·å–å½“å‰é…ç½®çš„LLMæä¾›å•†"""
        try:
            response = requests.get(f"{self.backend_url}/llm/provider", timeout=5)
            if response.status_code == 200:
                return response.json()
            else:
                print(f"âŒ è·å–æä¾›å•†å¤±è´¥: HTTP {response.status_code}")
                return []
        except Exception as e:
            print(f"âŒ è¿æ¥åç«¯å¤±è´¥: {e}")
            return []
    
    def show_provider_status(self):
        """æ˜¾ç¤ºæä¾›å•†çŠ¶æ€"""
        self.print_header("LLMæä¾›å•†çŠ¶æ€")
        
        current_providers = self.get_current_providers()
        
        if not current_providers:
            print("âŒ æ— æ³•è·å–æä¾›å•†ä¿¡æ¯ï¼Œè¯·ç¡®ä¿åç«¯æœåŠ¡è¿è¡Œæ­£å¸¸")
            return
        
        print(f"ğŸ“Š å·²é…ç½®çš„æä¾›å•†: {len(current_providers)} ä¸ª")
        print()
        
        for provider in current_providers:
            provider_name = provider.get('provider', 'unknown')
            display_name = provider.get('name', provider_name)
            default_model = provider.get('default_model_name', 'unknown')
            fast_model = provider.get('fast_default_model_name', 'unknown')
            api_key_set = provider.get('api_key_set', False)
            
            print(f"ğŸ”§ {display_name} ({provider_name})")
            print(f"   é»˜è®¤æ¨¡å‹: {default_model}")
            print(f"   å¿«é€Ÿæ¨¡å‹: {fast_model}")
            print(f"   APIå¯†é’¥: {'âœ… å·²é…ç½®' if api_key_set else 'âŒ æœªé…ç½®'}")
            
            # æ˜¾ç¤ºå¯ç”¨æ¨¡å‹
            model_configs = provider.get('model_configurations', [])
            if model_configs:
                visible_models = [m['name'] for m in model_configs if m.get('is_visible', False)]
                print(f"   å¯è§æ¨¡å‹: {len(visible_models)} ä¸ª")
                if visible_models:
                    print(f"   æ¨¡å‹åˆ—è¡¨: {', '.join(visible_models[:3])}{'...' if len(visible_models) > 3 else ''}")
            print()
    
    def show_supported_providers(self):
        """æ˜¾ç¤ºæ”¯æŒçš„æä¾›å•†"""
        self.print_header("æ”¯æŒçš„LLMæä¾›å•†")
        
        print("ğŸ“‹ Onyxæ”¯æŒä»¥ä¸‹LLMæä¾›å•†:")
        print()
        
        for provider_id, config in self.supported_providers.items():
            print(f"ğŸ”§ {config['display_name']} ({provider_id})")
            print(f"   APIå¯†é’¥: {'âœ… å¿…éœ€' if config.get('api_key_required') else 'âŒ å¯é€‰'}")
            
            if 'default_model' in config:
                print(f"   é»˜è®¤æ¨¡å‹: {config['default_model']}")
            if 'fast_model' in config:
                print(f"   å¿«é€Ÿæ¨¡å‹: {config['fast_model']}")
            
            print(f"   æ”¯æŒæ¨¡å‹: {len(config['models'])} ä¸ª")
            print(f"   ä¸»è¦æ¨¡å‹: {', '.join(config['models'][:3])}...")
            
            if 'custom_config' in config:
                print(f"   è‡ªå®šä¹‰é…ç½®: {', '.join(config['custom_config'])}")
            
            print()
    
    def generate_env_template(self):
        """ç”Ÿæˆç¯å¢ƒå˜é‡æ¨¡æ¿"""
        self.print_header("ç¯å¢ƒå˜é‡é…ç½®æ¨¡æ¿")
        
        print("ğŸ“ å°†ä»¥ä¸‹é…ç½®æ·»åŠ åˆ° .env æ–‡ä»¶:")
        print()
        
        print("# ===== OpenAI é…ç½® =====")
        print("GEN_AI_API_KEY=sk-your-openai-api-key-here")
        print("GEN_AI_MODEL_PROVIDER=openai")
        print("GEN_AI_MODEL_VERSION=gpt-4o")
        print("GEN_AI_TEMPERATURE=0.7")
        print()
        
        print("# ===== Anthropic é…ç½® =====")
        print("ANTHROPIC_API_KEY=your-anthropic-api-key-here")
        print()
        
        print("# ===== Azure OpenAI é…ç½® =====")
        print("AZURE_OPENAI_API_KEY=your-azure-openai-key")
        print("AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/")
        print("AZURE_OPENAI_API_VERSION=2024-02-01")
        print("AZURE_OPENAI_DEPLOYMENT_NAME=your-deployment-name")
        print()
        
        print("# ===== AWS Bedrock é…ç½® =====")
        print("AWS_REGION_NAME=us-east-1")
        print("AWS_ACCESS_KEY_ID=your-aws-access-key")
        print("AWS_SECRET_ACCESS_KEY=your-aws-secret-key")
        print()
        
        print("# ===== GCP Vertex AI é…ç½® =====")
        print("GOOGLE_APPLICATION_CREDENTIALS=/path/to/credentials.json")
        print("VERTEX_LOCATION=us-east1")
        print()
    
    def test_provider_connection(self, provider_name: str):
        """æµ‹è¯•æä¾›å•†è¿æ¥"""
        print(f"ğŸ” æµ‹è¯• {provider_name} è¿æ¥...")
        
        # è¿™é‡Œå¯ä»¥æ·»åŠ å…·ä½“çš„è¿æ¥æµ‹è¯•é€»è¾‘
        # ç”±äºéœ€è¦çœŸå®çš„APIå¯†é’¥ï¼Œè¿™é‡ŒåªåšåŸºç¡€æ£€æŸ¥
        
        if provider_name in self.supported_providers:
            config = self.supported_providers[provider_name]
            print(f"âœ… {config['display_name']} é…ç½®æ£€æŸ¥é€šè¿‡")
            
            # æ£€æŸ¥ç¯å¢ƒå˜é‡
            if provider_name == "openai":
                api_key = os.getenv("GEN_AI_API_KEY") or os.getenv("OPENAI_API_KEY")
                if api_key:
                    print("âœ… OpenAI APIå¯†é’¥å·²é…ç½®")
                else:
                    print("âŒ OpenAI APIå¯†é’¥æœªé…ç½®")
            
            elif provider_name == "anthropic":
                api_key = os.getenv("ANTHROPIC_API_KEY")
                if api_key:
                    print("âœ… Anthropic APIå¯†é’¥å·²é…ç½®")
                else:
                    print("âŒ Anthropic APIå¯†é’¥æœªé…ç½®")
            
            # å¯ä»¥æ·»åŠ æ›´å¤šæä¾›å•†çš„æ£€æŸ¥
            
        else:
            print(f"âŒ ä¸æ”¯æŒçš„æä¾›å•†: {provider_name}")
    
    def interactive_menu(self):
        """äº¤äº’å¼èœå•"""
        while True:
            print("\n" + "="*60)
            print("ğŸ¤– Onyx LLMé…ç½®ç®¡ç†å™¨")
            print("="*60)
            print("1. æŸ¥çœ‹å½“å‰æä¾›å•†çŠ¶æ€")
            print("2. æŸ¥çœ‹æ”¯æŒçš„æä¾›å•†")
            print("3. ç”Ÿæˆç¯å¢ƒå˜é‡æ¨¡æ¿")
            print("4. æµ‹è¯•æä¾›å•†è¿æ¥")
            print("5. æŸ¥çœ‹æ¨¡å‹é…ç½®")
            print("0. é€€å‡º")
            print("="*60)
            
            choice = input("è¯·é€‰æ‹©æ“ä½œ (0-5): ").strip()
            
            if choice == "0":
                print("ğŸ‘‹ é€€å‡ºLLMé…ç½®ç®¡ç†å™¨")
                break
            elif choice == "1":
                self.show_provider_status()
            elif choice == "2":
                self.show_supported_providers()
            elif choice == "3":
                self.generate_env_template()
            elif choice == "4":
                provider = input("è¯·è¾“å…¥æä¾›å•†åç§° (openai/anthropic/azure/bedrock/vertex_ai): ").strip()
                self.test_provider_connection(provider)
            elif choice == "5":
                self.show_model_configurations()
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡è¯•")
    
    def show_model_configurations(self):
        """æ˜¾ç¤ºæ¨¡å‹é…ç½®"""
        self.print_header("æ¨¡å‹é…ç½®è¯¦æƒ…")
        
        current_providers = self.get_current_providers()
        
        for provider in current_providers:
            provider_name = provider.get('name', 'unknown')
            print(f"ğŸ”§ {provider_name}")
            
            model_configs = provider.get('model_configurations', [])
            for model in model_configs:
                model_name = model.get('name', 'unknown')
                is_visible = model.get('is_visible', False)
                supports_vision = model.get('supports_image_input', False)
                max_tokens = model.get('max_input_tokens', 'unknown')
                
                visibility = "ğŸ‘ï¸ å¯è§" if is_visible else "ğŸ™ˆ éšè—"
                vision = "ğŸ–¼ï¸ æ”¯æŒå›¾åƒ" if supports_vision else "ğŸ“ ä»…æ–‡æœ¬"
                
                print(f"   ğŸ“± {model_name}")
                print(f"      {visibility} | {vision} | æœ€å¤§è¾“å…¥: {max_tokens}")
            print()

def main():
    """ä¸»å‡½æ•°"""
    manager = LLMConfigManager()
    
    print("ğŸ¤– Onyx LLMé…ç½®ç®¡ç†å™¨")
    print("ğŸ“… å¯åŠ¨æ—¶é—´:", time.strftime("%Y-%m-%d %H:%M:%S"))
    
    # å¯åŠ¨äº¤äº’å¼èœå•
    manager.interactive_menu()

if __name__ == "__main__":
    import time
    main()
