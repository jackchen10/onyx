# Onyxæ··åˆæ£€ç´¢ç®—æ³•å®ç°è¯¦è§£

## ğŸ¯ æ¦‚è¿°

æœ¬æ–‡æ¡£æ·±å…¥è§£æOnyxå·¥ç¨‹ä¸­æ··åˆæ£€ç´¢ç®—æ³•çš„æ ¸å¿ƒå®ç°ï¼Œè¯¦ç»†è¯´æ˜å‘é‡æ£€ç´¢ä¸å…¨æ–‡æ£€ç´¢çš„èåˆæœºåˆ¶ã€è¯„åˆ†ç®—æ³•å’Œä¼˜åŒ–ç­–ç•¥ã€‚

## ğŸ“Š æ··åˆæ£€ç´¢æ¶æ„å›¾

```mermaid
graph TB
    subgraph "æŸ¥è¯¢å¤„ç†"
        A[ç”¨æˆ·æŸ¥è¯¢] --> B[æŸ¥è¯¢é¢„å¤„ç†]
        B --> C[å…³é”®è¯æå–]
        B --> D[å‘é‡åŒ–]
    end
    
    subgraph "å¹¶è¡Œæ£€ç´¢"
        C --> E[BM25å…³é”®è¯æ£€ç´¢]
        D --> F[å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢]
        E --> G[å…³é”®è¯å€™é€‰é›†]
        F --> H[å‘é‡å€™é€‰é›†]
    end
    
    subgraph "æ··åˆè¯„åˆ†"
        G --> I[BM25åˆ†æ•°æ ‡å‡†åŒ–]
        H --> J[å‘é‡åˆ†æ•°æ ‡å‡†åŒ–]
        I --> K[åŠ æƒèåˆ]
        J --> K
        K --> L[æœ€ç»ˆæ’åº]
    end
    
    subgraph "åå¤„ç†"
        L --> M[æ—¶é—´è¡°å‡]
        M --> N[æ–‡æ¡£æå‡]
        N --> O[æƒé™è¿‡æ»¤]
        O --> P[è¿”å›ç»“æœ]
    end
```

## ğŸ” æ ¸å¿ƒç®—æ³•å®ç°

### 1. æ··åˆæ£€ç´¢ä¸»å‡½æ•°

<augment_code_snippet path="backend/onyx/document_index/vespa/index.py" mode="EXCERPT">
```python
def hybrid_retrieval(
    self,
    query: str,
    query_embedding: Embedding,
    final_keywords: list[str] | None,
    filters: IndexFilters,
    hybrid_alpha: float,  # å…³é”®å‚æ•°ï¼šæ··åˆæƒé‡
    time_decay_multiplier: float,
    num_to_retrieve: int,
    ranking_profile_type: QueryExpansionType,
    offset: int = 0,
    title_content_ratio: float | None = TITLE_CONTENT_RATIO,
) -> list[InferenceChunkUncleaned]:
```
</augment_code_snippet>

### 2. YQLæŸ¥è¯¢æ„å»ºç­–ç•¥

æ··åˆæ£€ç´¢é€šè¿‡æ„å»ºå¤åˆYQLæŸ¥è¯¢å®ç°å¤šè·¯å¾„å¹¶è¡Œæ£€ç´¢ï¼š

```python
# æ„å»ºæ··åˆæ£€ç´¢YQL
yql = (
    YQL_BASE.format(index_name=self.index_name)
    + vespa_where_clauses
    + f"(({{targetHits: {target_hits}}}nearestNeighbor(embeddings, query_embedding)) "
    + f"or ({{targetHits: {target_hits}}}nearestNeighbor(title_embedding, query_embedding)) "
    + 'or ({grammar: "weakAnd"}userInput(@query)) '
    + f'or ({{defaultIndex: "{CONTENT_SUMMARY}"}}userInput(@query)))'
)
```

**æŸ¥è¯¢è·¯å¾„è§£æ**ï¼š
1. `nearestNeighbor(embeddings, query_embedding)` - å†…å®¹å‘é‡æ£€ç´¢
2. `nearestNeighbor(title_embedding, query_embedding)` - æ ‡é¢˜å‘é‡æ£€ç´¢  
3. `{grammar: "weakAnd"}userInput(@query)` - å¼±ANDå…³é”®è¯æ£€ç´¢
4. `{defaultIndex: "content_summary"}userInput(@query)` - å†…å®¹æ‘˜è¦æ£€ç´¢

### 3. Ranking Profileé€‰æ‹©é€»è¾‘

```python
# æ ¹æ®æŸ¥è¯¢æ‰©å±•ç±»å‹é€‰æ‹©ranking profile
if ranking_profile_type == QueryExpansionType.KEYWORD:
    ranking_profile = f"hybrid_search_keyword_base_{len(query_embedding)}"
else:
    ranking_profile = f"hybrid_search_semantic_base_{len(query_embedding)}"
```

**Profileå·®å¼‚**ï¼š
- **è¯­ä¹‰ä¼˜å…ˆ**: first-phaseä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦ç­›é€‰
- **å…³é”®è¯ä¼˜å…ˆ**: first-phaseä½¿ç”¨BM25åˆ†æ•°ç­›é€‰

## ğŸ§® è¯„åˆ†ç®—æ³•è¯¦è§£

### 1. æ··åˆè¯„åˆ†å…¬å¼

Vespaä¸­çš„æ··åˆè¯„åˆ†åœ¨global-phaseé˜¶æ®µè®¡ç®—ï¼š

```vespa
global-phase {
    expression {
        (
            # å‘é‡ç›¸ä¼¼åº¦åˆ†æ•° (æƒé‡: Î±)
            query(alpha) * (
                (query(title_content_ratio) * normalize_linear(title_vector_score))
                +
                ((1 - query(title_content_ratio)) * normalize_linear(closeness(field, embeddings)))
            )
        )
        +
        # å…³é”®è¯ç›¸ä¼¼åº¦åˆ†æ•° (æƒé‡: 1-Î±)
        (
            (1 - query(alpha)) * (
                (query(title_content_ratio) * normalize_linear(bm25(title)))
                +
                ((1 - query(title_content_ratio)) * normalize_linear(bm25(content)))
            )
        )
    }
    # åº”ç”¨æå‡å› å­
    * document_boost
    * recency_bias  
    * aggregated_chunk_boost
}
```

### 2. è¯„åˆ†ç»„ä»¶åˆ†æ

#### å‘é‡ç›¸ä¼¼åº¦è®¡ç®—

```python
def calculate_vector_score(
    query_embedding: list[float],
    doc_embedding: list[float],
    title_embedding: list[float] | None,
    title_content_ratio: float
) -> float:
    """è®¡ç®—å‘é‡ç›¸ä¼¼åº¦åˆ†æ•°"""
    
    # å†…å®¹å‘é‡ç›¸ä¼¼åº¦
    content_similarity = cosine_similarity(query_embedding, doc_embedding)
    
    # æ ‡é¢˜å‘é‡ç›¸ä¼¼åº¦
    if title_embedding:
        title_similarity = cosine_similarity(query_embedding, title_embedding)
        # å–æœ€å¤§å€¼é¿å…æ— å…³æ ‡é¢˜å½±å“
        vector_score = max(content_similarity, title_similarity)
    else:
        vector_score = content_similarity
    
    # åŠ æƒç»„åˆ
    final_score = (
        title_content_ratio * title_similarity +
        (1 - title_content_ratio) * content_similarity
    )
    
    return final_score
```

#### BM25å…³é”®è¯è¯„åˆ†

```python
def calculate_bm25_score(
    query_terms: list[str],
    title: str,
    content: str,
    title_content_ratio: float,
    k1: float = 1.2,
    b: float = 0.75
) -> float:
    """è®¡ç®—BM25å…³é”®è¯è¯„åˆ†"""
    
    # æ ‡é¢˜BM25åˆ†æ•°
    title_score = bm25_score(query_terms, title, k1, b)
    
    # å†…å®¹BM25åˆ†æ•°
    content_score = bm25_score(query_terms, content, k1, b)
    
    # åŠ æƒç»„åˆ
    final_score = (
        title_content_ratio * title_score +
        (1 - title_content_ratio) * content_score
    )
    
    return final_score

def bm25_score(terms: list[str], text: str, k1: float, b: float) -> float:
    """æ ‡å‡†BM25è¯„åˆ†å®ç°"""
    # æ–‡æ¡£é•¿åº¦æ ‡å‡†åŒ–
    doc_length = len(text.split())
    avg_doc_length = 500  # å‡è®¾å¹³å‡æ–‡æ¡£é•¿åº¦
    
    score = 0.0
    for term in terms:
        tf = text.lower().count(term.lower())  # è¯é¢‘
        idf = calculate_idf(term)  # é€†æ–‡æ¡£é¢‘ç‡
        
        # BM25å…¬å¼
        term_score = idf * (tf * (k1 + 1)) / (
            tf + k1 * (1 - b + b * (doc_length / avg_doc_length))
        )
        score += term_score
    
    return score
```

### 3. åˆ†æ•°æ ‡å‡†åŒ–

```python
def normalize_linear(score: float, min_score: float = 0.0, max_score: float = 1.0) -> float:
    """çº¿æ€§æ ‡å‡†åŒ–åˆ°[0,1]åŒºé—´"""
    if max_score == min_score:
        return 0.0
    return (score - min_score) / (max_score - min_score)

def normalize_scores_batch(scores: list[float]) -> list[float]:
    """æ‰¹é‡æ ‡å‡†åŒ–åˆ†æ•°"""
    if not scores:
        return []
    
    min_score = min(scores)
    max_score = max(scores)
    
    return [normalize_linear(score, min_score, max_score) for score in scores]
```

## âš–ï¸ æƒé‡å‚æ•°ä¼˜åŒ–

### 1. Alphaå‚æ•°è°ƒä¼˜ç­–ç•¥

```python
class HybridAlphaOptimizer:
    """æ··åˆæ£€ç´¢Alphaå‚æ•°ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.query_patterns = {
            "factual": 0.3,      # äº‹å®æ€§æŸ¥è¯¢åå‘å…³é”®è¯
            "conceptual": 0.7,   # æ¦‚å¿µæ€§æŸ¥è¯¢åå‘è¯­ä¹‰
            "navigational": 0.2, # å¯¼èˆªæ€§æŸ¥è¯¢åå‘å…³é”®è¯
            "informational": 0.5 # ä¿¡æ¯æ€§æŸ¥è¯¢å¹³è¡¡
        }
    
    def optimize_alpha(
        self,
        query: str,
        query_type: str | None = None,
        user_feedback: dict | None = None
    ) -> float:
        """åŠ¨æ€ä¼˜åŒ–Alphaå‚æ•°"""
        
        # åŸºäºæŸ¥è¯¢ç±»å‹
        if query_type and query_type in self.query_patterns:
            base_alpha = self.query_patterns[query_type]
        else:
            base_alpha = self.classify_query_type(query)
        
        # åŸºäºæŸ¥è¯¢é•¿åº¦è°ƒæ•´
        query_length = len(query.split())
        if query_length <= 2:
            base_alpha *= 0.6  # çŸ­æŸ¥è¯¢åå‘å…³é”®è¯
        elif query_length >= 8:
            base_alpha *= 1.4  # é•¿æŸ¥è¯¢åå‘è¯­ä¹‰
        
        # åŸºäºç”¨æˆ·åé¦ˆè°ƒæ•´
        if user_feedback:
            base_alpha = self.adjust_by_feedback(base_alpha, user_feedback)
        
        return max(0.0, min(1.0, base_alpha))
    
    def classify_query_type(self, query: str) -> float:
        """è‡ªåŠ¨åˆ†ç±»æŸ¥è¯¢ç±»å‹"""
        query_lower = query.lower()
        
        # äº‹å®æ€§æŸ¥è¯¢ç‰¹å¾
        factual_keywords = ["what", "when", "where", "who", "how many"]
        if any(keyword in query_lower for keyword in factual_keywords):
            return 0.3
        
        # æ¦‚å¿µæ€§æŸ¥è¯¢ç‰¹å¾
        conceptual_keywords = ["why", "how", "explain", "concept", "theory"]
        if any(keyword in query_lower for keyword in conceptual_keywords):
            return 0.7
        
        # é»˜è®¤å¹³è¡¡
        return 0.5
```

### 2. æ ‡é¢˜å†…å®¹æ¯”ä¾‹ä¼˜åŒ–

```python
def optimize_title_content_ratio(source_type: str, document_structure: dict) -> float:
    """ä¼˜åŒ–æ ‡é¢˜å†…å®¹æ¯”ä¾‹"""
    
    # åŸºäºæ•°æ®æºç±»å‹çš„åŸºç¡€æ¯”ä¾‹
    base_ratios = {
        "confluence": 0.4,   # Wikiæ–‡æ¡£æ ‡é¢˜é‡è¦
        "notion": 0.4,       # ç¬”è®°æ–‡æ¡£æ ‡é¢˜é‡è¦
        "slack": 0.1,        # èŠå¤©æ¶ˆæ¯æ ‡é¢˜ä¸é‡è¦
        "email": 0.3,        # é‚®ä»¶æ ‡é¢˜ä¸­ç­‰é‡è¦
        "pdf": 0.5,          # PDFæ–‡æ¡£æ ‡é¢˜å¾ˆé‡è¦
        "web": 0.3,          # ç½‘é¡µæ ‡é¢˜ä¸­ç­‰é‡è¦
    }
    
    base_ratio = base_ratios.get(source_type, 0.3)
    
    # åŸºäºæ–‡æ¡£ç»“æ„è°ƒæ•´
    if document_structure:
        title_length = len(document_structure.get("title", ""))
        content_length = len(document_structure.get("content", ""))
        
        # æ ‡é¢˜è¿‡çŸ­æˆ–è¿‡é•¿æ—¶é™ä½æƒé‡
        if title_length < 10 or title_length > 200:
            base_ratio *= 0.7
        
        # å†…å®¹è¿‡çŸ­æ—¶æé«˜æ ‡é¢˜æƒé‡
        if content_length < 100:
            base_ratio *= 1.3
    
    return max(0.1, min(0.6, base_ratio))
```

## ğŸ”„ å¤šè·¯å¾„æ£€ç´¢èåˆ

### 1. æŸ¥è¯¢æ‰©å±•ç­–ç•¥

<augment_code_snippet path="backend/onyx/context/search/retrieval/search_runner.py" mode="EXCERPT">
```python
# å…³é”®è¯æ‰©å±•æ£€ç´¢
top_keyword_chunks_thread = run_in_background(
    document_index.hybrid_retrieval,
    query.expanded_queries.keywords_expansions[0],
    keyword_embeddings[0],
    query.processed_keywords,
    query.filters,
    HYBRID_ALPHA_KEYWORD,  # å…³é”®è¯æƒé‡0.4
    query.recency_bias_multiplier,
    query.num_hits,
    QueryExpansionType.KEYWORD,
    query.offset,
)

# è¯­ä¹‰æ‰©å±•æ£€ç´¢
top_semantic_chunks_thread = run_in_background(
    document_index.hybrid_retrieval,
    query.expanded_queries.semantic_expansions[0],
    semantic_embeddings[0],
    query.processed_keywords,
    query.filters,
    HYBRID_ALPHA,  # è¯­ä¹‰æƒé‡0.5
    query.recency_bias_multiplier,
    query.num_hits,
    QueryExpansionType.SEMANTIC,
    query.offset,
)
```
</augment_code_snippet>

### 2. ç»“æœèåˆç®—æ³•

```python
def fuse_retrieval_results(
    base_results: list[InferenceChunk],
    keyword_results: list[InferenceChunk],
    semantic_results: list[InferenceChunk] | None,
    fusion_method: str = "rrf"  # Reciprocal Rank Fusion
) -> list[InferenceChunk]:
    """èåˆå¤šè·¯å¾„æ£€ç´¢ç»“æœ"""
    
    if fusion_method == "rrf":
        return reciprocal_rank_fusion([base_results, keyword_results, semantic_results])
    elif fusion_method == "weighted":
        return weighted_fusion([base_results, keyword_results, semantic_results])
    else:
        return base_results

def reciprocal_rank_fusion(
    result_lists: list[list[InferenceChunk]],
    k: int = 60
) -> list[InferenceChunk]:
    """å€’æ•°æ’åèåˆç®—æ³•"""
    
    chunk_scores = {}
    
    for result_list in result_lists:
        if not result_list:
            continue
            
        for rank, chunk in enumerate(result_list, 1):
            chunk_id = chunk.document_id + "_" + str(chunk.chunk_id)
            
            # RRFåˆ†æ•°è®¡ç®—
            rrf_score = 1.0 / (k + rank)
            
            if chunk_id in chunk_scores:
                chunk_scores[chunk_id]["score"] += rrf_score
            else:
                chunk_scores[chunk_id] = {
                    "chunk": chunk,
                    "score": rrf_score
                }
    
    # æŒ‰èåˆåˆ†æ•°æ’åº
    sorted_chunks = sorted(
        chunk_scores.values(),
        key=lambda x: x["score"],
        reverse=True
    )
    
    return [item["chunk"] for item in sorted_chunks]
```

## ğŸ¯ æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 1. æŸ¥è¯¢ä¼˜åŒ–

```python
def optimize_query_performance(
    query: str,
    num_to_retrieve: int,
    filters: IndexFilters
) -> dict:
    """æŸ¥è¯¢æ€§èƒ½ä¼˜åŒ–"""
    
    # åŠ¨æ€è°ƒæ•´ç›®æ ‡å‘½ä¸­æ•°
    target_hits = max(10 * num_to_retrieve, 1000)
    
    # åŸºäºè¿‡æ»¤å™¨å¤æ‚åº¦è°ƒæ•´
    if len(filters.source_type or []) > 5:
        target_hits = min(target_hits, 2000)  # é™åˆ¶å¤æ‚è¿‡æ»¤æŸ¥è¯¢
    
    # åŸºäºæŸ¥è¯¢é•¿åº¦è°ƒæ•´
    query_terms = len(query.split())
    if query_terms > 10:
        target_hits = min(target_hits, 1500)  # é™åˆ¶é•¿æŸ¥è¯¢
    
    return {
        "target_hits": target_hits,
        "timeout": 30,
        "rerank_count": min(1000, target_hits),
    }
```

### 2. ç¼“å­˜ç­–ç•¥

```python
class HybridSearchCache:
    """æ··åˆæ£€ç´¢ç¼“å­˜"""
    
    def __init__(self, max_size: int = 1000):
        self.cache = {}
        self.max_size = max_size
        self.access_times = {}
    
    def get_cache_key(
        self,
        query: str,
        embedding: list[float],
        alpha: float,
        filters: IndexFilters
    ) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        embedding_hash = hash(tuple(embedding[:10]))  # ä½¿ç”¨å‰10ç»´
        filters_hash = hash(str(sorted(filters.__dict__.items())))
        
        return f"{hash(query)}_{embedding_hash}_{alpha}_{filters_hash}"
    
    def get(self, cache_key: str) -> list[InferenceChunk] | None:
        """è·å–ç¼“å­˜ç»“æœ"""
        if cache_key in self.cache:
            self.access_times[cache_key] = time.time()
            return self.cache[cache_key]
        return None
    
    def set(self, cache_key: str, results: list[InferenceChunk]):
        """è®¾ç½®ç¼“å­˜ç»“æœ"""
        if len(self.cache) >= self.max_size:
            self._evict_oldest()
        
        self.cache[cache_key] = results
        self.access_times[cache_key] = time.time()
    
    def _evict_oldest(self):
        """æ·˜æ±°æœ€æ—§çš„ç¼“å­˜é¡¹"""
        oldest_key = min(self.access_times.keys(), key=lambda k: self.access_times[k])
        del self.cache[oldest_key]
        del self.access_times[oldest_key]
```

## ğŸ“ˆ æ•ˆæœè¯„ä¼°æŒ‡æ ‡

### 1. æ£€ç´¢è´¨é‡æŒ‡æ ‡

```python
def evaluate_hybrid_retrieval(
    queries: list[str],
    ground_truth: dict,
    alpha_values: list[float]
) -> dict:
    """è¯„ä¼°æ··åˆæ£€ç´¢æ•ˆæœ"""
    
    results = {}
    
    for alpha in alpha_values:
        metrics = {
            "recall_at_10": [],
            "precision_at_10": [],
            "ndcg_at_10": [],
            "mrr": []
        }
        
        for query in queries:
            # æ‰§è¡Œæ£€ç´¢
            retrieved_docs = hybrid_retrieval(query, alpha=alpha)
            relevant_docs = ground_truth.get(query, [])
            
            # è®¡ç®—æŒ‡æ ‡
            metrics["recall_at_10"].append(
                calculate_recall_at_k(retrieved_docs, relevant_docs, 10)
            )
            metrics["precision_at_10"].append(
                calculate_precision_at_k(retrieved_docs, relevant_docs, 10)
            )
            metrics["ndcg_at_10"].append(
                calculate_ndcg_at_k(retrieved_docs, relevant_docs, 10)
            )
            metrics["mrr"].append(
                calculate_mrr(retrieved_docs, relevant_docs)
            )
        
        # è®¡ç®—å¹³å‡å€¼
        results[alpha] = {
            metric: sum(values) / len(values)
            for metric, values in metrics.items()
        }
    
    return results

def calculate_recall_at_k(retrieved: list, relevant: list, k: int) -> float:
    """è®¡ç®—Recall@K"""
    retrieved_k = set(retrieved[:k])
    relevant_set = set(relevant)
    
    if not relevant_set:
        return 0.0
    
    return len(retrieved_k & relevant_set) / len(relevant_set)

def calculate_ndcg_at_k(retrieved: list, relevant: list, k: int) -> float:
    """è®¡ç®—NDCG@K"""
    # å®ç°NDCGè®¡ç®—é€»è¾‘
    pass
```

### 2. æ€§èƒ½ç›‘æ§

```python
class HybridRetrievalMonitor:
    """æ··åˆæ£€ç´¢æ€§èƒ½ç›‘æ§"""
    
    def __init__(self):
        self.metrics = {
            "query_count": 0,
            "total_latency": 0.0,
            "alpha_distribution": {},
            "error_count": 0,
        }
    
    def record_query(
        self,
        query: str,
        alpha: float,
        latency: float,
        success: bool,
        result_count: int
    ):
        """è®°å½•æŸ¥è¯¢æŒ‡æ ‡"""
        self.metrics["query_count"] += 1
        self.metrics["total_latency"] += latency
        
        # Alphaåˆ†å¸ƒç»Ÿè®¡
        alpha_bucket = round(alpha, 1)
        self.metrics["alpha_distribution"][alpha_bucket] = (
            self.metrics["alpha_distribution"].get(alpha_bucket, 0) + 1
        )
        
        if not success:
            self.metrics["error_count"] += 1
    
    def get_performance_report(self) -> dict:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        total_queries = self.metrics["query_count"]
        
        return {
            "avg_latency": self.metrics["total_latency"] / max(total_queries, 1),
            "error_rate": self.metrics["error_count"] / max(total_queries, 1),
            "queries_per_second": total_queries / 3600,  # å‡è®¾1å°æ—¶ç»Ÿè®¡
            "alpha_distribution": self.metrics["alpha_distribution"],
        }
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2025-02-19  
**é€‚ç”¨ç‰ˆæœ¬**: Onyx v1.0+
