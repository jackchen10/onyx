# Onyxæ¡†æ¶é›†æˆQwen3ã€Kimi K2å’ŒDeepSeekæ¨¡å‹æŒ‡å—

## ğŸ¯ ç›®æ ‡

åœ¨Onyxæ¡†æ¶ä¸­æ·»åŠ å¯¹Qwen3ã€Kimi K2å’ŒDeepSeekæ¨¡å‹çš„æ”¯æŒï¼Œåˆ©ç”¨ç°æœ‰çš„LiteLLMæ¶æ„å®ç°æ— ç¼é›†æˆã€‚

## ğŸ” ç°æœ‰LLMæ¶æ„åˆ†æ

### ğŸ“Š Onyx LLMæ¶æ„æ¦‚è§ˆ

Onyxä½¿ç”¨**LiteLLM**ä½œä¸ºç»Ÿä¸€çš„LLMæ¥å£å±‚ï¼Œæ”¯æŒå¤šç§LLMæä¾›å•†ï¼š

```python
# æ ¸å¿ƒæ¶æ„
DefaultMultiLLM (onyx.llm.chat_llm)
    â†“
LiteLLM Library (ç»Ÿä¸€æ¥å£)
    â†“
å„ç§LLMæä¾›å•† (OpenAI, Anthropic, Azure, Bedrock, Vertex AIç­‰)
```

### ğŸ—ï¸ å…³é”®ç»„ä»¶

#### 1. LLMå·¥å‚ (`backend/onyx/llm/factory.py`)
```python
def get_llm(
    provider: str,           # æä¾›å•†åç§°
    model: str,             # æ¨¡å‹åç§°
    max_input_tokens: int,  # æœ€å¤§è¾“å…¥token
    api_key: str | None,    # APIå¯†é’¥
    api_base: str | None,   # APIåŸºç¡€URL
    # ... å…¶ä»–å‚æ•°
) -> LLM:
    return DefaultMultiLLM(
        model_provider=provider,
        model_name=model,
        # ... é…ç½®å‚æ•°
    )
```

#### 2. æä¾›å•†é…ç½® (`backend/onyx/llm/llm_provider_options.py`)
```python
# ç°æœ‰æä¾›å•†é…ç½®ç¤ºä¾‹
OPENAI_PROVIDER_NAME = "openai"
ANTHROPIC_PROVIDER_NAME = "anthropic"
BEDROCK_PROVIDER_NAME = "bedrock"
VERTEXAI_PROVIDER_NAME = "vertex_ai"

# æä¾›å•†åˆ°æ¨¡å‹çš„æ˜ å°„
_PROVIDER_TO_MODELS_MAP = {
    OPENAI_PROVIDER_NAME: OPEN_AI_MODEL_NAMES,
    ANTHROPIC_PROVIDER_NAME: ANTHROPIC_MODEL_NAMES,
    # ...
}
```

#### 3. DefaultMultiLLM (`backend/onyx/llm/chat_llm.py`)
```python
class DefaultMultiLLM(LLM):
    def _completion(self, ...):
        return litellm.completion(
            model=f"{self.config.model_provider}/{self.config.model_name}",
            api_key=self._api_key,
            base_url=self._api_base,
            # ... å…¶ä»–å‚æ•°
        )
```

## ğŸš€ LiteLLMå¯¹Qwenå’ŒKimiçš„æ”¯æŒ

### âœ… éªŒè¯ç»“æœ

é€šè¿‡æ£€æŸ¥LiteLLMï¼Œç¡®è®¤æ”¯æŒä»¥ä¸‹æ¨¡å‹ï¼š

#### ğŸ¤– Qwenæ¨¡å‹æ”¯æŒ
```python
# LiteLLMæ”¯æŒçš„Qwenæ¨¡å‹ (éƒ¨åˆ†åˆ—è¡¨)
qwen_models = [
    'openrouter/qwen/qwen-2.5-coder-32b-instruct',
    'openrouter/qwen/qwen-vl-plus', 
    'openrouter/qwen/qwen3-coder',
    'groq/qwen/qwen3-32b',
    'cerebras/qwen-3-32b',
    'sambanova/Qwen3-32B',
    'nscale/Qwen/Qwen2.5-Coder-32B-Instruct',
    # ... æ›´å¤šæ¨¡å‹
]
```

#### ğŸŒ™ Kimiæ¨¡å‹æ”¯æŒ
```python
# LiteLLMæ”¯æŒçš„Kimiæ¨¡å‹
kimi_models = [
    'groq/moonshotai/kimi-k2-instruct',
    'moonshot/moonshot-v1-8k',
    'moonshot/moonshot-v1-32k',
    'moonshot/moonshot-v1-128k',
]
```

#### ğŸ§  DeepSeekæ¨¡å‹æ”¯æŒ
```python
# LiteLLMæ”¯æŒçš„DeepSeekæ¨¡å‹
deepseek_models = [
    'deepseek/deepseek-r1',           # DeepSeek R1 (æ¨ç†æ¨¡å‹)
    'deepseek/deepseek-v3',           # DeepSeek V3 (æœ€æ–°ç‰ˆæœ¬)
    'deepseek/deepseek-chat',         # DeepSeek Chat
    'deepseek/deepseek-coder',        # DeepSeek Coder (ä»£ç ä¸“ç”¨)
    'deepseek/deepseek-reasoner',     # DeepSeek Reasoner (æ¨ç†ä¸“ç”¨)
    'openrouter/deepseek/deepseek-r1', # é€šè¿‡OpenRouter
    'azure_ai/deepseek-r1',           # é€šè¿‡Azure AI
]
```

## ğŸ”§ é›†æˆæ–¹æ¡ˆ

### æ–¹æ¡ˆ1: é€šè¿‡ç°æœ‰æä¾›å•†é›†æˆ (æ¨è)

#### 1.1 ä½¿ç”¨OpenRouteré›†æˆQwen3
```python
# åœ¨llm_provider_options.pyä¸­æ·»åŠ 
OPENROUTER_PROVIDER_NAME = "openrouter"
OPENROUTER_MODEL_NAMES = [
    "qwen/qwen3-coder",
    "qwen/qwen-2.5-coder-32b-instruct",
    "qwen/qwen-vl-plus",
]
OPENROUTER_DEFAULT_MODEL = "qwen/qwen3-coder"

# æ·»åŠ åˆ°æä¾›å•†æ˜ å°„
_PROVIDER_TO_MODELS_MAP[OPENROUTER_PROVIDER_NAME] = OPENROUTER_MODEL_NAMES
```

#### 1.2 ä½¿ç”¨Groqé›†æˆKimi K2
```python
# æ‰©å±•ç°æœ‰Groqé…ç½®
GROQ_PROVIDER_NAME = "groq"  # å¯èƒ½å·²å­˜åœ¨
GROQ_MODEL_NAMES = [
    # ç°æœ‰Groqæ¨¡å‹...
    "moonshotai/kimi-k2-instruct",  # æ·»åŠ Kimi K2
    "qwen/qwen3-32b",              # æ·»åŠ Qwen3
]
```

### æ–¹æ¡ˆ2: æ·»åŠ æ–°çš„æä¾›å•†

#### 2.1 æ·»åŠ Qwenæä¾›å•†
```python
# åœ¨llm_provider_options.pyä¸­æ·»åŠ 
QWEN_PROVIDER_NAME = "qwen"
QWEN_MODEL_NAMES = [
    "qwen3-coder",
    "qwen-2.5-coder-32b-instruct", 
    "qwen-vl-plus",
]
QWEN_DEFAULT_MODEL = "qwen3-coder"
QWEN_DEFAULT_FAST_MODEL = "qwen3-coder"

# æ·»åŠ åˆ°é…ç½®å‡½æ•°
def fetch_available_well_known_llms():
    return [
        # ... ç°æœ‰æä¾›å•†
        WellKnownLLMProviderDescriptor(
            name=QWEN_PROVIDER_NAME,
            display_name="Qwen (é€šä¹‰åƒé—®)",
            api_key_required=True,
            api_base_required=True,  # éœ€è¦è‡ªå®šä¹‰APIç«¯ç‚¹
            api_version_required=False,
            custom_config_keys=[],
            model_configurations=fetch_model_configurations_for_provider(QWEN_PROVIDER_NAME),
            default_model=QWEN_DEFAULT_MODEL,
            default_fast_model=QWEN_DEFAULT_FAST_MODEL,
        ),
    ]
```

#### 2.2 æ·»åŠ Moonshot (Kimi)æä¾›å•†
```python
MOONSHOT_PROVIDER_NAME = "moonshot"
MOONSHOT_MODEL_NAMES = [
    "kimi-k2-instruct",
    "moonshot-v1-8k",
    "moonshot-v1-32k",
    "moonshot-v1-128k",
]
MOONSHOT_DEFAULT_MODEL = "kimi-k2-instruct"
MOONSHOT_VISIBLE_MODEL_NAMES = ["kimi-k2-instruct"]
```

#### 2.3 æ·»åŠ DeepSeekæä¾›å•†
```python
DEEPSEEK_PROVIDER_NAME = "deepseek"
DEEPSEEK_MODEL_NAMES = [
    "deepseek-r1",           # DeepSeek R1 (æ¨ç†æ¨¡å‹)
    "deepseek-v3",           # DeepSeek V3 (æœ€æ–°ç‰ˆæœ¬)
    "deepseek-chat",         # DeepSeek Chat
    "deepseek-coder",        # DeepSeek Coder (ä»£ç ä¸“ç”¨)
    "deepseek-reasoner",     # DeepSeek Reasoner (æ¨ç†ä¸“ç”¨)
]
DEEPSEEK_DEFAULT_MODEL = "deepseek-r1"
DEEPSEEK_VISIBLE_MODEL_NAMES = ["deepseek-r1", "deepseek-v3", "deepseek-coder"]
```

## ğŸ“ å…·ä½“å®ç°æ­¥éª¤

### æ­¥éª¤1: ä¿®æ”¹æä¾›å•†é…ç½®

<function_calls>
<invoke name="view">
<parameter name="path">backend/onyx/llm/llm_provider_options.py
